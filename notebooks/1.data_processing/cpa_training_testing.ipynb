{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header files loaded!\n"
     ]
    }
   ],
   "source": [
    "# header files\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import math\n",
    "import tensorboard\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "print(\"Header files loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPA Training\n",
    "The CPA Training required the QC_Image.csv file which had the 21 features ['ImageQuality_Correlation_10', 'ImageQuality_Correlation_20', 'ImageQuality_Correlation_5', 'ImageQuality_Correlation_50', 'ImageQuality_FocusScore', 'ImageQuality_LocalFocusScore_10', 'ImageQuality_LocalFocusScore_20', 'ImageQuality_LocalFocusScore_5', 'ImageQuality_LocalFocusScore_50', 'ImageQuality_MADIntensity', 'ImageQuality_MaxIntensity', 'ImageQuality_MeanIntensity', 'ImageQuality_MedianIntensity', 'ImageQuality_MinIntensity', 'ImageQuality_PercentMaximal', 'ImageQuality_PercentMinimal', 'ImageQuality_PowerLogLogSlope', 'ImageQuality_Scaling', 'ImageQuality_StdIntensity', 'ImageQuality_TotalArea' and 'ImageQuality_TotalIntensity'] for the each image used in the training dataset and validation dataset. Three models were tried which will be described in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jupyter-arpit@broadinstitu-ef612/data/broad_annotations/QC_Image.csv']\n"
     ]
    }
   ],
   "source": [
    "# cpa features for the training dataset\n",
    "files = glob.glob(\"/home/jupyter-arpit@broadinstitu-ef612/data/broad_annotations/QC_Image.csv\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2060\n",
      "2060\n",
      "400\n",
      "400\n",
      "560\n",
      "600\n",
      "350\n",
      "550\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# get the cpa features for the training and validation dataset\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "train_count_good = 0\n",
    "train_count_blur = 0\n",
    "train_count_empty = 0\n",
    "train_count_debris = 0\n",
    "val_count_good = 0\n",
    "val_count_blur = 0\n",
    "val_count_empty = 0\n",
    "val_count_debris = 0\n",
    "for file in files:\n",
    "    \n",
    "    flag = -1\n",
    "    with open(file, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile)\n",
    "        for row in spamreader:\n",
    "            if flag == -1:\n",
    "                array = row\n",
    "                flag = 1\n",
    "            else:\n",
    "                array = row\n",
    "                \n",
    "                file = array[35]\n",
    "                if \"train\" in file:\n",
    "                    # feature vector\n",
    "                    train_sample = []\n",
    "                    for index in range(12, 33):\n",
    "                        train_sample.append(array[index])\n",
    "                    X_train.append(train_sample)\n",
    "                    \n",
    "                    # label\n",
    "                    if \"good\" in file:\n",
    "                        Y_train.append(0)\n",
    "                        train_count_good += 1\n",
    "                    elif \"blur\" in file:\n",
    "                        Y_train.append(1)\n",
    "                        train_count_blur += 1\n",
    "                    elif \"empty\" in file:\n",
    "                        Y_train.append(2)\n",
    "                        train_count_empty += 1\n",
    "                    else:\n",
    "                        Y_train.append(3)\n",
    "                        train_count_debris += 1\n",
    "                else:\n",
    "                    # feature vector\n",
    "                    val_sample = []\n",
    "                    for index in range(12, 33):\n",
    "                        val_sample.append(array[index])\n",
    "                    X_val.append(val_sample)\n",
    "                    \n",
    "                    # label\n",
    "                    if \"good\" in file:\n",
    "                        Y_val.append(0)\n",
    "                        val_count_good += 1\n",
    "                    elif \"blur\" in file:\n",
    "                        Y_val.append(1)\n",
    "                        val_count_blur += 1\n",
    "                    elif \"empty\" in file:\n",
    "                        Y_val.append(2)\n",
    "                        val_count_empty += 1\n",
    "                    else:\n",
    "                        Y_val.append(3)\n",
    "                        val_count_debris += 1\n",
    "                        \n",
    "\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(X_val))\n",
    "print(len(Y_val))\n",
    "print(train_count_good)\n",
    "print(train_count_blur)\n",
    "print(train_count_empty)\n",
    "print(train_count_debris)\n",
    "print(val_count_good)\n",
    "print(val_count_blur)\n",
    "print(val_count_empty)\n",
    "print(val_count_debris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPA Models\n",
    "The three models used were: Random Forest, AdaBoost and Gradient Boost. The best performing model on the validation dataset was used on the test dataset.<br><br>\n",
    "The input to the CPA Model -> 21 features of image<br>\n",
    "The output from the CPA Model -> one of the four classes [good, blurry, empty and debris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random-forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# ada-boost classifier\n",
    "#rfc = AdaBoostClassifier()\n",
    "#rfc.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# gradient boosting classifier\n",
    "#rfc = GradientBoostingClassifier()\n",
    "#rfc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 2 3 3 3 3 3 1 1 3 1 1 1 3 1 1 1 1 1 1 3 3 3 0 3 1 0 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 0 3 3 3 3 3 3 3 0 3 3 3 3 3 3 0 3 0 3 3 3 3 3 3 3 3 0 0 3 2 3 3 3\n",
      " 3 3 3 0 3 2 3 0 3 3 3 2 0 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2\n",
      " 2 2 2 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 3 0 3 0 0 0 0 0 0 3 3 0 3 0 0 0 0 3 0 0 0 0 0 0 0 2 0 0 0 0\n",
      " 0 3 0 0 0 0 0 3 2 0 0 0 0 0 3 0 0 0 0 0 0 1 3 2 2 3 3 3 1 3]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# prediction on validation dataset\n",
    "pred = rfc.predict(X_val)\n",
    "print(pred)\n",
    "print(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "[[79  3  4 14]\n",
      " [ 5 86  1  8]\n",
      " [ 1  1 95  3]\n",
      " [11  3  3 83]]\n"
     ]
    }
   ],
   "source": [
    "# accuracy and confusion matrix on validation dataset\n",
    "correct = 0\n",
    "for index in range(0, 400):\n",
    "    if pred[index] == Y_val[index]:\n",
    "        correct += 1\n",
    "print(correct)\n",
    "print(confusion_matrix(Y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CPA Model\n",
    "The best performing model was saved in .sav format. Then it will be loaded and the performance will be analysed on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "filename = '../qc_bestmodel_cpa.sav'\n",
    "pickle.dump(rfc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPA Model on Test Dataset\n",
    "The next task was to test the performance of the CPA model on the test dataset. The process was three step fold:\n",
    "1. Load the CPA model\n",
    "2. Get features for each image in the test dataset\n",
    "3. Use the CPA model for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "filename = '../qc_bestmodel_cpa.sav'\n",
    "rfc = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17277\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "files = glob.glob(\"/dgx1nas1/cellpainting-datasets/2019_07_11_JUMP_CP_pilots/workspace/qc/2021_03_03_Stain5_CondC_PE_Standard/results/*/*/*_Image.csv\")\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86385\n",
      "86385\n"
     ]
    }
   ],
   "source": [
    "test_files = []\n",
    "X_test = []\n",
    "for file in files:\n",
    "    \n",
    "    flag = -1\n",
    "    indexes_dna = []\n",
    "    indexes_rna = []\n",
    "    indexes_er = []\n",
    "    indexes_agp = []\n",
    "    indexes_mito = []\n",
    "    index_filedna = -1\n",
    "    index_filerna = -1\n",
    "    index_fileer = -1\n",
    "    index_fileagp = -1\n",
    "    index_filemito = -1\n",
    "    with open(file, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile)\n",
    "        for row in spamreader:\n",
    "            if flag == -1:\n",
    "                array = row\n",
    "                flag = 1\n",
    "                for index in range(0, len(array)):\n",
    "                    if \"URL_OrigDNA\" in array[index]:\n",
    "                        index_filedna = index\n",
    "                    if \"URL_OrigRNA\" in array[index]:\n",
    "                        index_filerna = index\n",
    "                    if \"URL_OrigER\" in array[index]:\n",
    "                        index_fileer = index\n",
    "                    if \"URL_OrigMito\" in array[index]:\n",
    "                        index_filemito = index\n",
    "                    if \"URL_OrigAGP\" in array[index]:\n",
    "                        index_fileagp = index\n",
    "                        \n",
    "                        \n",
    "                    # append dna features\n",
    "                    if \"OrigDNA\" in array[index] and \"ImageQuality\" in array[index]:\n",
    "                        indexes_dna.append(index)\n",
    "                    \n",
    "                    # append rna features\n",
    "                    if \"OrigRNA\" in array[index] and \"ImageQuality\" in array[index]:\n",
    "                        indexes_rna.append(index)\n",
    "                        \n",
    "                    # append er features\n",
    "                    if \"OrigER\" in array[index] and \"ImageQuality\" in array[index]:\n",
    "                        indexes_er.append(index)\n",
    "                        \n",
    "                    # append agp features\n",
    "                    if \"OrigAGP\" in array[index] and \"ImageQuality\" in array[index]:\n",
    "                        indexes_agp.append(index)\n",
    "                        \n",
    "                    # append mito features\n",
    "                    if \"OrigMito\" in array[index] and \"ImageQuality\" in array[index]:\n",
    "                        indexes_mito.append(index)\n",
    "            else:\n",
    "                array = row\n",
    "                \n",
    "                # dna file\n",
    "                if index_filedna >= 0:\n",
    "                    file_dna = \"/dgx1nas1/cellpainting-datasets/\"\n",
    "                    sample = array[index_filedna].split(\"/\")\n",
    "                    for index in range(5, len(sample)):\n",
    "                        file_dna = file_dna + sample[index]\n",
    "                        file_dna = file_dna + \"/\"\n",
    "                    file_dna = file_dna.replace(\"2019_07_11_JUMP-CP-pilots\", \"2019_07_11_JUMP_CP_pilots\")\n",
    "                    test_files.append(file_dna[:len(file_dna)-1])\n",
    "                    \n",
    "                    sample_dna = []\n",
    "                    for index in range(0, len(indexes_dna)):\n",
    "                        sample_dna.append(array[indexes_dna[index]])\n",
    "                    X_test.append(sample_dna)\n",
    "                    \n",
    "                # rna file\n",
    "                if index_filerna >= 0:\n",
    "                    file_rna = \"/dgx1nas1/cellpainting-datasets/\"\n",
    "                    sample = array[index_filerna].split(\"/\")\n",
    "                    for index in range(5, len(sample)):\n",
    "                        file_rna = file_rna + sample[index]\n",
    "                        file_rna = file_rna + \"/\"\n",
    "                    file_rna = file_rna.replace(\"2019_07_11_JUMP-CP-pilots\", \"2019_07_11_JUMP_CP_pilots\")\n",
    "                    test_files.append(file_rna[:len(file_rna)-1])\n",
    "                    \n",
    "                    sample_rna = []\n",
    "                    for index in range(0, len(indexes_rna)):\n",
    "                        sample_rna.append(array[indexes_rna[index]])\n",
    "                    X_test.append(sample_rna)\n",
    "                    \n",
    "                # er file\n",
    "                if index_fileer >= 0:\n",
    "                    file_er = \"/dgx1nas1/cellpainting-datasets/\"\n",
    "                    sample = array[index_fileer].split(\"/\")\n",
    "                    for index in range(5, len(sample)):\n",
    "                        file_er = file_er + sample[index]\n",
    "                        file_er = file_er + \"/\"\n",
    "                    file_er = file_er.replace(\"2019_07_11_JUMP-CP-pilots\", \"2019_07_11_JUMP_CP_pilots\")\n",
    "                    test_files.append(file_er[:len(file_er)-1])\n",
    "                    \n",
    "                    sample_er = []\n",
    "                    for index in range(0, len(indexes_er)):\n",
    "                        sample_er.append(array[indexes_er[index]])\n",
    "                    X_test.append(sample_er)\n",
    "                    \n",
    "                # agp file\n",
    "                if index_fileagp >= 0:\n",
    "                    file_agp = \"/dgx1nas1/cellpainting-datasets/\"\n",
    "                    sample = array[index_fileagp].split(\"/\")\n",
    "                    for index in range(5, len(sample)):\n",
    "                        file_agp = file_agp + sample[index]\n",
    "                        file_agp = file_agp + \"/\"\n",
    "                    file_agp = file_agp.replace(\"2019_07_11_JUMP-CP-pilots\", \"2019_07_11_JUMP_CP_pilots\")\n",
    "                    test_files.append(file_agp[:len(file_agp)-1])\n",
    "                    \n",
    "                    sample_agp = []\n",
    "                    for index in range(0, len(indexes_agp)):\n",
    "                        sample_agp.append(array[indexes_agp[index]])\n",
    "                    X_test.append(sample_agp)\n",
    "                    \n",
    "                # mito file\n",
    "                if index_filemito >= 0:\n",
    "                    file_mito = \"/dgx1nas1/cellpainting-datasets/\"\n",
    "                    sample = array[index_filemito].split(\"/\")\n",
    "                    for index in range(5, len(sample)):\n",
    "                        file_mito = file_mito + sample[index]\n",
    "                        file_mito = file_mito + \"/\"\n",
    "                    file_mito = file_mito.replace(\"2019_07_11_JUMP-CP-pilots\", \"2019_07_11_JUMP_CP_pilots\")\n",
    "                    test_files.append(file_mito[:len(file_mito)-1])\n",
    "                    \n",
    "                    sample_mito = []\n",
    "                    for index in range(0, len(indexes_mito)):\n",
    "                        sample_mito.append(array[indexes_mito[index]])\n",
    "                    X_test.append(sample_mito)\n",
    "print(len(X_test))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "pred = rfc.predict(X_test)\n",
    "pred_prob = rfc.predict_proba(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions\n",
    "After running the CPA model through the test dataset, the next task was to use the predictions on each image of the test dataset and check the count for each class and save it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61509\n",
      "0\n",
      "54\n",
      "24822\n"
     ]
    }
   ],
   "source": [
    "count_good = 0\n",
    "count_blur = 0\n",
    "count_empty = 0\n",
    "count_debris = 0\n",
    "for index in range(0, len(pred)):\n",
    "    if pred[index] == 0:\n",
    "        count_good += 1\n",
    "    elif pred[index] == 1:\n",
    "        count_blur += 1\n",
    "    elif pred[index] == 2:\n",
    "        count_empty += 1\n",
    "    elif pred[index] == 3:\n",
    "        count_debris += 1\n",
    "\n",
    "print(count_good)\n",
    "print(count_blur)\n",
    "print(count_empty)\n",
    "print(count_debris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86385\n"
     ]
    }
   ],
   "source": [
    "# write results\n",
    "count = 0\n",
    "with open(\"test_baseline_cpa.csv\", 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow([\"File Path\", \"Good Probability\", \"Blurry Probability\", \"Empty Probability\", \"Debris Probability\", \"Label\"])\n",
    "    for index in range(0, len(pred)):\n",
    "        if pred[index] == 1 or pred[index] == 2 or pred[index] == 3 or pred[index] == 0:\n",
    "            count += 1\n",
    "            spamwriter.writerow([test_files[index], pred_prob[index][0], pred_prob[index][1], pred_prob[index][2], pred_prob[index][3], pred[index]])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
